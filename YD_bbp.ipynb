{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dd88af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take-home assignment for Machine Learning Scientist position\n",
    "# check if all necessary packages have been installed: pandas, sklearn, tensorflow, shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b96adff",
   "metadata": {},
   "source": [
    "Summary: <br>\n",
    "    1. Assumptions：<br>\n",
    "        day-to-day and fermenter-to-fermenter variablity don't affect the OD600. <br>\n",
    "    2. Approach：<br>\n",
    "        a. Data gathering and preprocessing:<br>\n",
    "        Interpolated OD600 to generate more data for model training. Aligned the online parameters with the corresponding offline OD600 timestamps to build traning data and training sequences of length time_step = 10 for LSTMs model training for next step. Features are standardized to prevent dominance of one features. <br>\n",
    "        b. Model Setup <br>\n",
    "        LSTMs (Long Short-Term Memory networks) has been used as the basic model architecture since it excels at capturing temporal dependencies within time-series data. Built a univariate-output LSTM model to predict OD600 from multiple online features. Used mean squared error (MSE) as the loss function. Tried dropout (0.2–0.3) and varied the number of LSTM units (e.g., 50 → 20) to combat overfitting. Split the dataset into training and validation sets (e.g., 90%/10%) without shuffling (time-series respect). <br>\n",
    "        c. Model Training and Hyperparameter Tuning <br>\n",
    "        Ran multiple epochs (e.g., 20–50). Observed training loss vs. validation loss to diagnose overfitting. Adjusted dropout, the number of hidden units, and tried smaller or larger learning rates to improve generalization.<br>\n",
    "        d. Prediction of Missing OD600 Data <br>\n",
    "        After training, used the model to infer the OD600 time-series trendline for fermenters T3-080724, T6-080724, T7-120224, and T8-120224 (which were automatically found by comparing the missing exp_id between offline OD data file and online params file). <br>\n",
    "        e. Feature Importance Analysis <br>\n",
    "        SHAP (SHapley Additive exPlanations), a powerful method used to interpret the predictions of machine learning models, especially neural networks. Here, the GradientExplainer has been used to find the feature importance.<br>   \n",
    "    3. Conclusion：<br>\n",
    "    a. Model Predictablity:<br>\n",
    "        The LSTM model learned to approximate biomass growth trends reasonably, but it showed signs of overfitting, evidenced by a large gap between training loss and validation loss. Nevertheless, within certain intervals, the model could capture OD600 trajectories with moderate fidelity. The predictability is constrained by OD600 generation with interpolation. The data makes it challenging to capture the real OD trend.  <br>\n",
    "    b. Key Causal Features <br> \n",
    "        By applying SHAP, I observed that pH_control_buffer_total and feed_total onsistently showed higher attribution scores for OD600 prediction. These features collectively influenced how the model tracked biomass growth. <br>\n",
    "    c. Practical Implications <br>\n",
    "        The model’s ability to forecast biomass could assist in real-time monitoring and decision-making for fermentation processes, but additional data (more offline OD points) and refined regularization are recommended to improve robustness and reduce overfitting. <br>\n",
    "    d. Additional questions (Is the assumption reasonable?) <br>\n",
    "        The reason of overfitting is likely to come from the assumption which ignores the affects of fermenters and days。 Fermentation microbes may undergo mutations during their division and growth, leading to different batches exhibiting different OD trends. Euipment evolutions can also cause unexpected change to OD trends. My model has ability to distinguish 'exp_id'. To alter the model to quality the question, one way worth to try is to add categorical indicators as new features for day and dermenter, concatenate these with my usual time-series inputs before feeding them into the LSTM, and then analyze the learned embeddings to see how OD600 predictions shift across different days and fermenters. More advanced approach could be using a hierarchical or mixed-effects method if you want formal random effects and more structured interpretability.   \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b27c3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data preprocessing module\n",
    "import data_preprocessing\n",
    "import LSTM_Regression\n",
    "import Predict_missing_data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e34fcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10060\\Downloads\\Boston_Bioprocess\\data_preprocessing.py:70: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  interpolated_group = numeric_group.interpolate(method='linear')\n",
      "C:\\Users\\10060\\Downloads\\Boston_Bioprocess\\data_preprocessing.py:70: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  interpolated_group = numeric_group.interpolate(method='linear')\n",
      "C:\\Users\\10060\\Downloads\\Boston_Bioprocess\\data_preprocessing.py:70: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  interpolated_group = numeric_group.interpolate(method='linear')\n",
      "C:\\Users\\10060\\Downloads\\Boston_Bioprocess\\data_preprocessing.py:70: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  interpolated_group = numeric_group.interpolate(method='linear')\n",
      "C:\\Users\\10060\\Downloads\\Boston_Bioprocess\\data_preprocessing.py:70: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  interpolated_group = numeric_group.interpolate(method='linear')\n",
      "C:\\Users\\10060\\Downloads\\Boston_Bioprocess\\data_preprocessing.py:70: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  interpolated_group = numeric_group.interpolate(method='linear')\n",
      "C:\\Users\\10060\\Downloads\\Boston_Bioprocess\\data_preprocessing.py:70: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  interpolated_group = numeric_group.interpolate(method='linear')\n",
      "C:\\Users\\10060\\Downloads\\Boston_Bioprocess\\data_preprocessing.py:70: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  interpolated_group = numeric_group.interpolate(method='linear')\n",
      "C:\\Users\\10060\\Downloads\\Boston_Bioprocess\\data_preprocessing.py:70: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  interpolated_group = numeric_group.interpolate(method='linear')\n",
      "C:\\Users\\10060\\Downloads\\Boston_Bioprocess\\data_preprocessing.py:70: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  interpolated_group = numeric_group.interpolate(method='linear')\n",
      "C:\\Users\\10060\\Downloads\\Boston_Bioprocess\\data_preprocessing.py:70: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  interpolated_group = numeric_group.interpolate(method='linear')\n",
      "C:\\Users\\10060\\Downloads\\Boston_Bioprocess\\data_preprocessing.py:70: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  interpolated_group = numeric_group.interpolate(method='linear')\n",
      "C:\\Users\\10060\\Downloads\\Boston_Bioprocess\\data_preprocessing.py:70: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  interpolated_group = numeric_group.interpolate(method='linear')\n",
      "C:\\Users\\10060\\Downloads\\Boston_Bioprocess\\data_preprocessing.py:70: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  interpolated_group = numeric_group.interpolate(method='linear')\n",
      "C:\\Users\\10060\\Downloads\\Boston_Bioprocess\\data_preprocessing.py:70: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  interpolated_group = numeric_group.interpolate(method='linear')\n",
      "C:\\Users\\10060\\Downloads\\Boston_Bioprocess\\data_preprocessing.py:70: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  interpolated_group = numeric_group.interpolate(method='linear')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# File paths to all data\n",
    "online_params_file_1 = 'Time_series_online_params.csv'\n",
    "online_params_file_2 = 'Timeseries_run_online_params_120224.csv'\n",
    "od_data_file = 'Time_series_OD_data.csv'\n",
    "\n",
    "# Get columns name for features and target\n",
    "feature_cols = ['Agitation','DO', 'feed_flowrate', 'feed_total', 'pH', 'pH_control_buffer_flowrate', 'pH_control_buffer_total', 'product_inducer_flowrate', 'product_inducer_total']\n",
    "target_col = ['OD600']\n",
    "\n",
    "df_online_1 = data_preprocessing.load_online_params(online_params_file_1)\n",
    "df_online_2 = data_preprocessing.load_online_params(online_params_file_2)\n",
    "# Concatenate the DataFrames\n",
    "df_online = pd.concat([df_online_1, df_online_2], axis=0).reset_index(drop=True)\n",
    "df_od = data_preprocessing.load_od_data(od_data_file)\n",
    "\n",
    "# Preprocess data to get standardized dataset with exp_id, time_hr, features, and target\n",
    "preprocessed_data, scalar = data_preprocessing.preprocess_data(df_online, df_od, feature_cols, target_col)\n",
    "# print(preprocessed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19356835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target\n",
    "features = preprocessed_data[feature_cols].to_numpy()\n",
    "target = preprocessed_data[target_col].to_numpy()\n",
    "\n",
    "# Define the number of timesteps for sequences\n",
    "time_steps = 10\n",
    "\n",
    "# Create sequences for LSTM\n",
    "X, y = LSTM_Regression.create_sequences_for_multiple_series(preprocessed_data, 'exp_id', feature_cols, target_col, time_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b23032d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10060\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1670/1670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 6039.7915 - val_loss: 489.4005\n",
      "Epoch 2/20\n",
      "\u001b[1m1670/1670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 918.1878 - val_loss: 624.2961\n",
      "Epoch 3/20\n",
      "\u001b[1m1670/1670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 844.2747 - val_loss: 700.8958\n",
      "Epoch 4/20\n",
      "\u001b[1m1670/1670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 817.4377 - val_loss: 649.6716\n",
      "Epoch 5/20\n",
      "\u001b[1m1670/1670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 738.6742 - val_loss: 838.0960\n",
      "Epoch 6/20\n",
      "\u001b[1m1670/1670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 689.4185 - val_loss: 1146.2966\n",
      "Epoch 7/20\n",
      "\u001b[1m1670/1670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 685.0648 - val_loss: 779.8785\n",
      "Epoch 8/20\n",
      "\u001b[1m1670/1670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 666.0085 - val_loss: 1424.4464\n",
      "Epoch 9/20\n",
      "\u001b[1m1670/1670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 637.8809 - val_loss: 1403.5656\n",
      "Epoch 10/20\n",
      "\u001b[1m1670/1670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 614.9761 - val_loss: 981.4994\n",
      "Epoch 11/20\n",
      "\u001b[1m1670/1670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 601.3454 - val_loss: 1872.8658\n",
      "Epoch 12/20\n",
      "\u001b[1m1670/1670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 581.0250 - val_loss: 2017.6475\n",
      "Epoch 13/20\n",
      "\u001b[1m1670/1670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 556.7758 - val_loss: 1132.5365\n",
      "Epoch 14/20\n",
      "\u001b[1m1670/1670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 549.4353 - val_loss: 1202.8090\n",
      "Epoch 15/20\n",
      "\u001b[1m1670/1670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 537.2216 - val_loss: 982.2154\n",
      "Epoch 16/20\n",
      "\u001b[1m1670/1670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 548.0221 - val_loss: 938.8770\n",
      "Epoch 17/20\n",
      "\u001b[1m1670/1670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 535.6285 - val_loss: 931.8391\n",
      "Epoch 18/20\n",
      "\u001b[1m1670/1670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 518.8353 - val_loss: 1859.2322\n",
      "Epoch 19/20\n",
      "\u001b[1m1670/1670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 516.0729 - val_loss: 832.6927\n",
      "Epoch 20/20\n",
      "\u001b[1m1670/1670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 507.0225 - val_loss: 805.4656\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Train the LSTM model\n",
    "Regression_model, mistory_model, mse, X_train, X_val = LSTM_Regression.LSTM_model(X,y,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b428b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the exp_id which have missing OD values\n",
    "exp_id_x_missing = Predict_missing_data.find_exp_id_for_x_missing(df_online, df_od)\n",
    "# print(exp_id_x_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8742a1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features for missing data prediction\n",
    "dfs = []\n",
    "for series_id in exp_id_x_missing:\n",
    "    df_series = df_online[df_online['exp_id'] == series_id]\n",
    "    standardize_df_series = data_preprocessing.standardize_data_only_features(df_series, feature_cols)\n",
    "    dfs.append(standardize_df_series)\n",
    "Standardized_missing_online = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a6b4372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate input sequences for the new standardized feature data\n",
    "feature_cols = ['Agitation','DO', 'feed_flowrate', 'feed_total', 'pH', 'pH_control_buffer_flowrate', 'pH_control_buffer_total', 'product_inducer_flowrate', 'product_inducer_total']\n",
    "target_col = ['OD600']\n",
    "X_missing, seq_metadata = LSTM_Regression.create_sequences_for_multiple_series_inference(\n",
    "    df=Standardized_missing_online,\n",
    "    id_col=\"exp_id\",\n",
    "    feature_cols=feature_cols,\n",
    "    time_steps=time_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67a3aa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Get the predicted OD\n",
    "predictions = Predict_missing_data.predict_and_postprocess(Regression_model, X_missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9506230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions and metadata to a DataFrame for further use\n",
    "predictions_df = pd.DataFrame(seq_metadata)\n",
    "predictions_df['predicted_OD600'] = predictions.flatten()\n",
    "# print(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7be59853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10060\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor\n",
      "Received: inputs=['Tensor(shape=(10, 10, 9))']\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\10060\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor\n",
      "Received: inputs=['Tensor(shape=(50, 10, 9))']\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Find the feature importance\n",
    "feature_importance = Predict_missing_data.SHAP_feature_importance(X_train, X_val, Regression_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baa8428e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0: [1.73955991]\n",
      "Feature 1: [0.33163232]\n",
      "Feature 2: [0.43152321]\n",
      "Feature 3: [1.93808462]\n",
      "Feature 4: [0.35149593]\n",
      "Feature 5: [0.2316468]\n",
      "Feature 6: [4.27552404]\n",
      "Feature 7: [1.53940063]\n",
      "Feature 8: [0.66083893]\n"
     ]
    }
   ],
   "source": [
    "# To get feature-only importance (not considering time series importance):\n",
    "import numpy as np\n",
    "\n",
    "shap_values_agg_time = np.mean(np.abs(feature_importance), axis=1)    # remove time\n",
    "# shap_values_agg_time.shape -> (n_samples, n_features)\n",
    "\n",
    "feature_importance_final = np.mean(shap_values_agg_time, axis=0)     # remove samples\n",
    "# feature_importance.shape -> (n_features,)\n",
    "\n",
    "for i, score in enumerate(feature_importance_final):\n",
    "    print(f\"Feature {i}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ef1aab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pH_control_buffer_total  with  4.275524036139251\n",
      "feed_total  with  1.9380846180065419\n"
     ]
    }
   ],
   "source": [
    "# Extract the values from the lists and combine them with their indices\n",
    "feature_importances = [(i, score[0]) for i, score in enumerate(feature_importance_final)]\n",
    "\n",
    "# Sort the features by their importance scores in descending order\n",
    "sorted_features = sorted(feature_importances, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the top two features with the highest importance values\n",
    "top_two_features = sorted_features[:2]\n",
    "\n",
    "# Print the two most important features\n",
    "for i, score in top_two_features:\n",
    "    print(feature_cols[i], \" with \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197323d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
